<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Glassbox 2.0 â€” Mechanistic Interpretability</title>
  <meta name="description"
        content="Glassbox 2.0: open-source mechanistic interpretability for transformer language models. Identifies minimal faithful circuits in three forward passes." />

  <!-- OG / social -->
  <meta property="og:title"       content="Glassbox 2.0 â€” Mechanistic Interpretability" />
  <meta property="og:description" content="Find the minimal neural circuit responsible for any model prediction â€” in three forward passes." />
  <meta property="og:type"        content="website" />

  <style>
    /* â”€â”€ reset & tokens â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    *, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }

    :root {
      --bg:      #0f1117;
      --surface: #1a1d2e;
      --card:    #1e2130;
      --border:  #2d3561;
      --accent:  #3b82f6;
      --accent2: #8b5cf6;
      --green:   #10b981;
      --yellow:  #f59e0b;
      --red:     #ef4444;
      --text:    #e2e8f0;
      --muted:   #94a3b8;
      --code-bg: #0d1117;
      --radius:  10px;
    }

    html { scroll-behavior: smooth; }

    body {
      background: var(--bg);
      color: var(--text);
      font-family: 'Segoe UI', system-ui, -apple-system, sans-serif;
      line-height: 1.6;
    }

    /* â”€â”€ layout â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .container { max-width: 900px; margin: 0 auto; padding: 0 24px; }

    section { padding: 72px 0; }
    section:nth-child(even) { background: var(--surface); }

    h1 { font-size: clamp(2rem, 5vw, 3.2rem); font-weight: 700; line-height: 1.15; }
    h2 { font-size: 1.9rem; font-weight: 700; margin-bottom: 28px; }
    h3 { font-size: 1.15rem; font-weight: 600; margin-bottom: 8px; }

    p  { color: var(--muted); margin-bottom: 12px; }
    a  { color: var(--accent); text-decoration: none; }
    a:hover { text-decoration: underline; }

    /* â”€â”€ nav â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    nav {
      position: sticky; top: 0; z-index: 100;
      background: rgba(15,17,23,0.92);
      backdrop-filter: blur(12px);
      border-bottom: 1px solid var(--border);
      padding: 14px 0;
    }
    .nav-inner {
      display: flex; align-items: center; justify-content: space-between;
      max-width: 900px; margin: 0 auto; padding: 0 24px;
    }
    .nav-logo { font-weight: 700; font-size: 1.05rem; color: var(--text); }
    .nav-logo span { color: var(--accent); }
    .nav-links { display: flex; gap: 24px; }
    .nav-links a { color: var(--muted); font-size: 0.9rem; transition: color .2s; }
    .nav-links a:hover { color: var(--text); text-decoration: none; }

    /* â”€â”€ hero â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    #hero { padding: 100px 0 80px; text-align: center; }
    .hero-tag {
      display: inline-block;
      background: rgba(59,130,246,.15);
      color: var(--accent);
      border: 1px solid rgba(59,130,246,.35);
      border-radius: 999px;
      padding: 5px 16px;
      font-size: 0.82rem;
      font-weight: 600;
      letter-spacing: .04em;
      text-transform: uppercase;
      margin-bottom: 24px;
    }
    #hero h1 { margin-bottom: 20px; }
    #hero p  { font-size: 1.1rem; max-width: 640px; margin: 0 auto 36px; }

    .cta-group { display: flex; gap: 14px; justify-content: center; flex-wrap: wrap; }
    .btn {
      display: inline-flex; align-items: center; gap: 8px;
      padding: 12px 26px; border-radius: var(--radius);
      font-weight: 600; font-size: 0.95rem; transition: opacity .2s;
      cursor: pointer; border: none;
    }
    .btn:hover { opacity: .85; text-decoration: none; }
    .btn-primary  { background: var(--accent); color: #fff; }
    .btn-ghost    { background: transparent; color: var(--text); border: 1px solid var(--border); }
    .btn-purple   { background: var(--accent2); color: #fff; }

    /* â”€â”€ stats strip â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .stats-strip {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(180px, 1fr));
      gap: 1px;
      background: var(--border);
      border: 1px solid var(--border);
      border-radius: var(--radius);
      overflow: hidden;
      margin-top: 56px;
    }
    .stat {
      background: var(--card);
      padding: 28px 20px;
      text-align: center;
    }
    .stat-val  { font-size: 2rem; font-weight: 700; color: var(--accent); }
    .stat-label{ font-size: 0.78rem; color: var(--muted); margin-top: 4px; }
    .stat-note { font-size: 0.7rem; color: #64748b; margin-top: 4px; font-style: italic; }

    /* â”€â”€ disclaimer box â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .disclosure {
      background: rgba(245,158,11,.08);
      border: 1px solid rgba(245,158,11,.35);
      border-radius: var(--radius);
      padding: 18px 22px;
      margin: 32px 0;
      font-size: 0.88rem;
      color: #fcd34d;
    }
    .disclosure strong { display: block; margin-bottom: 4px; }

    /* â”€â”€ feature grid â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .feature-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
      gap: 20px;
    }
    .card {
      background: var(--card);
      border: 1px solid var(--border);
      border-radius: var(--radius);
      padding: 28px;
      transition: border-color .2s;
    }
    .card:hover { border-color: var(--accent); }
    .card-icon { font-size: 1.8rem; margin-bottom: 14px; }

    /* â”€â”€ how it works â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .pipeline {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
      gap: 20px;
      counter-reset: step;
    }
    .step {
      background: var(--card);
      border: 1px solid var(--border);
      border-radius: var(--radius);
      padding: 24px;
      position: relative;
    }
    .step::before {
      counter-increment: step;
      content: counter(step);
      position: absolute; top: -14px; left: 20px;
      background: var(--accent); color: #fff;
      width: 28px; height: 28px;
      border-radius: 50%;
      font-weight: 700; font-size: 0.82rem;
      display: flex; align-items: center; justify-content: center;
    }
    .step h3 { margin-top: 8px; }

    /* â”€â”€ metrics table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .metrics-table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.9rem;
    }
    .metrics-table th {
      text-align: left;
      padding: 12px 16px;
      background: var(--card);
      color: var(--muted);
      font-weight: 600;
      border-bottom: 1px solid var(--border);
    }
    .metrics-table td {
      padding: 14px 16px;
      border-bottom: 1px solid var(--border);
      vertical-align: top;
    }
    .metrics-table tr:last-child td { border-bottom: none; }
    .metrics-table code {
      background: var(--code-bg);
      border-radius: 4px;
      padding: 2px 6px;
      font-size: 0.82rem;
      color: #7dd3fc;
    }
    .tag-approx {
      display: inline-block;
      background: rgba(245,158,11,.15);
      color: var(--yellow);
      border-radius: 4px;
      padding: 1px 8px;
      font-size: 0.75rem;
      font-weight: 600;
    }
    .tag-exact {
      display: inline-block;
      background: rgba(16,185,129,.15);
      color: var(--green);
      border-radius: 4px;
      padding: 1px 8px;
      font-size: 0.75rem;
      font-weight: 600;
    }

    /* â”€â”€ benchmark â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .bench-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
      gap: 16px;
    }
    .bench-card {
      background: var(--card);
      border: 1px solid var(--border);
      border-radius: var(--radius);
      padding: 22px;
    }
    .bench-val  { font-size: 1.6rem; font-weight: 700; color: var(--green); }
    .bench-label{ font-size: 0.8rem; color: var(--muted); margin-top: 4px; }
    .bench-ci   { font-size: 0.72rem; color: #64748b; margin-top: 2px; }

    /* â”€â”€ code block â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    pre {
      background: var(--code-bg);
      border: 1px solid var(--border);
      border-radius: var(--radius);
      padding: 24px;
      overflow-x: auto;
      font-size: 0.87rem;
      line-height: 1.7;
    }
    pre code { color: #e2e8f0; }
    .kw  { color: #c084fc; }
    .fn  { color: #60a5fa; }
    .str { color: #86efac; }
    .cm  { color: #64748b; }
    .num { color: #fb923c; }

    /* â”€â”€ citations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    .citations ol { padding-left: 22px; }
    .citations li { color: var(--muted); font-size: 0.88rem; margin-bottom: 10px; }
    .citations li span { color: var(--text); font-weight: 500; }

    /* â”€â”€ footer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    footer {
      border-top: 1px solid var(--border);
      padding: 40px 0;
      text-align: center;
      color: var(--muted);
      font-size: 0.85rem;
    }
    footer a { color: var(--accent); }

    /* â”€â”€ responsive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
    @media (max-width: 600px) {
      .nav-links { display: none; }
      .stats-strip { grid-template-columns: 1fr 1fr; }
    }
  </style>
</head>

<body>

<!-- â”€â”€ NAV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
<nav>
  <div class="nav-inner">
    <div class="nav-logo">Glass<span>box</span> <span style="color:var(--muted);font-weight:400">2.0</span></div>
    <div class="nav-links">
      <a href="#how-it-works">How it works</a>
      <a href="#metrics">Metrics</a>
      <a href="#benchmarks">Benchmarks</a>
      <a href="#quickstart">Quickstart</a>
      <a href="#citations">References</a>
    </div>
  </div>
</nav>

<!-- â”€â”€ HERO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
<section id="hero">
  <div class="container">
    <div class="hero-tag">Open-source Â· MIT License Â· 2026</div>
    <h1>Mechanistic Interpretability<br>for Transformer Models</h1>
    <p>
      Glassbox 2.0 identifies the minimal faithful neural circuit responsible for any
      model prediction â€” using just <strong style="color:var(--text)">three forward passes</strong>
      for attribution scoring, with no architectural modifications required.
    </p>

    <div class="cta-group">
      <a class="btn btn-primary"
         href="https://huggingface.co/spaces/designer-coderajay/Glassbox-ai">
        ğŸš€ Live Demo (HF Spaces)
      </a>
      <a class="btn btn-ghost"
         href="https://github.com/designer-coderajay/Glassbox-AI-2.0-Mechanistic-Interpretability-tool">
        â­ GitHub
      </a>
      <a class="btn btn-purple"
         href="https://pypi.org/project/glassbox-mech-interp/">
        ğŸ“¦ pip install
      </a>
    </div>

    <!-- â”€â”€ stats â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
    <div class="stats-strip">
      <div class="stat">
        <div class="stat-val">3</div>
        <div class="stat-label">Forward passes for attribution scores</div>
        <div class="stat-note">attribution_patching() only; full analyze() = O(3 + 2p)</div>
      </div>
      <div class="stat">
        <div class="stat-val">37Ã—</div>
        <div class="stat-label">Faster wall-clock vs ACDC on GPT-2</div>
        <div class="stat-note">Measured end-to-end; ACDC uses edge-level search</div>
      </div>
      <div class="stat">
        <div class="stat-val">80%</div>
        <div class="stat-label">IOI sufficiency on GPT-2-small</div>
        <div class="stat-note">95% CI: [0.74, 0.87] over 20 prompts</div>
      </div>
      <div class="stat">
        <div class="stat-val">0.81</div>
        <div class="stat-label">Cross-model FCAS (GPT-2 vs GPT-2-medium)</div>
        <div class="stat-note">Range 0.783â€“0.835; above null 0.50 Â± 0.09</div>
      </div>
    </div>

    <div class="disclosure" style="margin-top:32px;text-align:left;">
      <strong>âš ï¸ Approximation note</strong>
      Sufficiency is computed via a first-order Taylor approximation
      (Nanda et al. 2023). It is accurate when circuit contributions are small
      relative to the clean logit-difference, but may overestimate when the
      circuit accounts for nearly all of the logit-difference. F1 inherits this
      approximation. Comprehensiveness is computed exactly via causal patching.
    </div>
  </div>
</section>

<!-- â”€â”€ HOW IT WORKS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
<section id="how-it-works">
  <div class="container">
    <h2>How It Works</h2>
    <p>
      Glassbox implements <strong style="color:var(--text)">attribution patching</strong>
      (Nanda et al. 2023) â€” a first-order Taylor approximation to activation patching
      that requires only three forward passes to score every attention head in the model.
      A greedy two-phase search then finds the smallest circuit that remains faithful.
    </p>

    <div class="pipeline" style="margin-top:36px;">
      <div class="step">
        <h3>Corrupt the prompt</h3>
        <p>Generate a corrupted prompt via bidirectional name-swap (e.g. Maryâ†”John)
           so the model's prediction changes meaningfully.</p>
      </div>
      <div class="step">
        <h3>Attribution patching</h3>
        <p>Score each attention head in <strong style="color:var(--text)">3 forward passes</strong>
           using the gradient of the logit-difference w.r.t. patched activations.
           Complexity: O(3). This is the fast step.</p>
      </div>
      <div class="step">
        <h3>Forward selection (approx.)</h3>
        <p>Greedily add top-scoring heads until sufficiency â‰¥ 0.85.
           Uses the Taylor approximation â€” no extra forward passes.</p>
      </div>
      <div class="step">
        <h3>Backward pruning (exact)</h3>
        <p>Remove each head one-by-one and recompute comprehensiveness exactly
           via causal patching (2 passes per candidate). Full analyze() = O(3 + 2p)
           where p = pruning steps.</p>
      </div>
      <div class="step">
        <h3>Faithfulness scoring</h3>
        <p>Report Comprehensiveness (exact) and Sufficiency (Taylor approximation)
           with a confidence interval via percentile bootstrap over n â‰¥ 20 prompts.</p>
      </div>
      <div class="step">
        <h3>Logit lens + FCAS</h3>
        <p>Visualise how predictions form across layers (nostalgebraist 2020).
           Compare circuits across models using FCAS with a null distribution.</p>
      </div>
    </div>

    <!-- pass complexity callout -->
    <div style="background:var(--card);border:1px solid var(--border);border-radius:var(--radius);padding:22px;margin-top:32px;">
      <h3 style="color:var(--accent);margin-bottom:12px;">Pass complexity â€” what "3 forward passes" means</h3>
      <table class="metrics-table">
        <tr>
          <th>Method</th><th>Step</th><th>Passes</th><th>Notes</th>
        </tr>
        <tr>
          <td><code>attribution_patching()</code></td>
          <td>Score all heads</td>
          <td style="color:var(--green);font-weight:700">O(3)</td>
          <td>Clean forward, corrupted forward, backward gradient</td>
        </tr>
        <tr>
          <td><code>analyze()</code></td>
          <td>Full pipeline</td>
          <td style="color:var(--yellow);font-weight:700">O(3 + 2p)</td>
          <td>3 attr passes + 2 passes per backward pruning step p</td>
        </tr>
        <tr>
          <td>ACDC (Conmy et al. 2023)</td>
          <td>Edge-level search</td>
          <td style="color:var(--red);font-weight:700">O(288+)</td>
          <td>Iterates over all edges; finer granularity</td>
        </tr>
      </table>
      <p style="margin-top:12px;font-size:0.82rem;">
        The 37Ã— wall-clock speedup measures <em>end-to-end circuit discovery</em> on GPT-2-small
        against ACDC's reported runtime. Pass counts are not directly comparable because
        the methods operate at different granularities (head-level vs edge-level).
      </p>
    </div>
  </div>
</section>

<!-- â”€â”€ METRICS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
<section id="metrics">
  <div class="container">
    <h2>Metrics Reference</h2>
    <p>
      All metrics follow Wang et al. (2022) terminology unless noted.
      Understanding approximation status is critical for correct interpretation.
    </p>

    <table class="metrics-table" style="margin-top:28px;">
      <thead>
        <tr>
          <th>Metric</th>
          <th>Formula</th>
          <th>Type</th>
          <th>Interpretation</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>Logit Difference (LD)</strong></td>
          <td><code>logit(correct) âˆ’ logit(incorrect)</code></td>
          <td><span class="tag-exact">Exact</span></td>
          <td>
            How much the model prefers the correct token.
            Baseline for all other metrics. Higher = more confident.
          </td>
        </tr>
        <tr>
          <td><strong>Sufficiency</strong></td>
          <td><code>â‰ˆ Î£ attr(circuit) / LD_clean</code></td>
          <td><span class="tag-approx">Approx. (Taylor)</span></td>
          <td>
            Does the circuit alone explain the prediction?
            <em>First-order Taylor approximation</em> â€” accurate when circuit
            contributions are small; can overestimate near LD_circuit â‰ˆ LD_clean.
            Increasing this threshold makes circuits smaller but less reliable.
          </td>
        </tr>
        <tr>
          <td><strong>Comprehensiveness</strong></td>
          <td><code>1 âˆ’ LD_patched / LD_clean</code></td>
          <td><span class="tag-exact">Exact</span></td>
          <td>
            Does removing the circuit break the model? Computed by patching
            circuit heads with corrupted activations. Exact causal intervention,
            no approximation. Higher = circuit is truly necessary.
          </td>
        </tr>
        <tr>
          <td><strong>F1</strong></td>
          <td><code>2Â·SuffÂ·Comp / (Suff + Comp)</code></td>
          <td><span class="tag-approx">Approx. (inherits Suff)</span></td>
          <td>
            Harmonic mean â€” penalises circuits that are sufficient but not
            comprehensive, or vice versa. Inherits the Taylor approximation
            from Sufficiency. Category thresholds: faithful â‰¥ 0.85,
            backup_mechanisms â‰¥ 0.70, moderate â‰¥ 0.50, incomplete â‰¥ 0.30.
          </td>
        </tr>
        <tr>
          <td><strong>FCAS</strong></td>
          <td><code>1 âˆ’ mean(|depth_A_i âˆ’ depth_B_i|)</code></td>
          <td><span class="tag-approx">Rank-match approx.</span></td>
          <td>
            Cross-model circuit alignment. Compares relative depth of top-k
            matched head pairs. Range [0, 1]; 1 = identical structure.
            <strong>Limitation:</strong> rank-matching is not functional
            equivalence; depth alone ignores head index. Always compare against
            the null distribution (random circuits), which averages ~0.50 Â± 0.09.
          </td>
        </tr>
        <tr>
          <td><strong>Bootstrap CI</strong></td>
          <td><code>[P2.5, P97.5]</code> over n prompts</td>
          <td><span class="tag-exact">Exact (percentile)</span></td>
          <td>
            Percentile bootstrap with 1000 resamples. Requires <strong>n â‰¥ 20
            prompts</strong> for reliable confidence intervals. Below n=10,
            intervals are too narrow to be trusted.
          </td>
        </tr>
      </tbody>
    </table>
  </div>
</section>

<!-- â”€â”€ BENCHMARKS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
<section id="benchmarks">
  <div class="container">
    <h2>Benchmark Results</h2>
    <p>
      All benchmarks run on GPT-2-small (117M) on CPU, averaged over â‰¥ 10 prompts
      per task. Speed measured end-to-end wall-clock vs ACDC's reported runtimes
      (Conmy et al. 2023) on equivalent hardware.
    </p>

    <div class="bench-grid" style="margin-top:28px;">
      <div class="bench-card">
        <div class="bench-val">0.80</div>
        <div class="bench-label">IOI Sufficiency (GPT-2-small)</div>
        <div class="bench-ci">95% CI: [0.74, 0.87] Â· n=20 Â· Taylor approx.</div>
      </div>
      <div class="bench-card">
        <div class="bench-val">0.73</div>
        <div class="bench-label">IOI Comprehensiveness</div>
        <div class="bench-ci">95% CI: [0.68, 0.79] Â· n=20 Â· Exact</div>
      </div>
      <div class="bench-card">
        <div class="bench-val">0.76</div>
        <div class="bench-label">IOI F1</div>
        <div class="bench-ci">Harmonic mean; inherits Suff approximation</div>
      </div>
      <div class="bench-card">
        <div class="bench-val">37Ã—</div>
        <div class="bench-label">Wall-clock speedup vs ACDC</div>
        <div class="bench-ci">End-to-end circuit discovery Â· GPT-2-small</div>
      </div>
      <div class="bench-card">
        <div class="bench-val">0.81</div>
        <div class="bench-label">FCAS â€” GPT-2 vs GPT-2-medium</div>
        <div class="bench-ci">Range 0.783â€“0.835 Â· Null baseline: 0.50 Â± 0.09 Â· k=10</div>
      </div>
      <div class="bench-card">
        <div class="bench-val">3</div>
        <div class="bench-label">Forward passes for attribution scoring</div>
        <div class="bench-ci">Full analyze() = O(3 + 2p) where p = pruning steps</div>
      </div>
    </div>

    <div class="disclosure" style="margin-top:28px;">
      <strong>âš ï¸ Benchmark caveats</strong>
      These numbers are from a single run on one hardware configuration. Sufficiency
      values are Taylor approximations â€” not exact causal measurements. FCAS
      measures rank-aligned depth similarity, not functional equivalence.
      Bootstrap CIs use n=20 prompts; wider intervals expected at n&lt;10.
    </div>
  </div>
</section>

<!-- â”€â”€ QUICKSTART â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
<section id="quickstart">
  <div class="container">
    <h2>Quickstart</h2>
    <p>Three lines to find the minimal circuit responsible for a model's prediction.</p>

    <pre><code><span class="cm"># Install</span>
pip install glassbox-mech-interp

<span class="cm"># Analyse any prompt</span>
<span class="kw">from</span> glassbox <span class="kw">import</span> GlassboxV2

gb     = GlassboxV2(<span class="str">"gpt2"</span>)         <span class="cm"># loads GPT-2-small</span>
result = gb.analyze(
    prompt    = <span class="str">"When Mary and John went to the store, John gave a drink to"</span>,
    correct   = <span class="str">"Mary"</span>,
    incorrect = <span class="str">"John"</span>,
)

<span class="cm"># Circuit: [(layer, head), ...] sorted by attribution score</span>
<span class="fn">print</span>(result[<span class="str">"circuit"</span>])

<span class="cm"># Faithfulness metrics</span>
faith = result[<span class="str">"faithfulness"</span>]
<span class="fn">print</span>(<span class="str">f"Sufficiency (approx):  {faith['sufficiency']:.3f}"</span>)  <span class="cm"># Taylor approx.</span>
<span class="fn">print</span>(<span class="str">f"Comprehensiveness:     {faith['comprehensiveness']:.3f}"</span>)  <span class="cm"># exact</span>
<span class="fn">print</span>(<span class="str">f"F1:                   {faith['f1']:.3f}"</span>)
<span class="fn">print</span>(<span class="str">f"Category:             {faith['category']}"</span>)
<span class="cm"># â†’ e.g. "faithful" | "backup_mechanisms" | "moderate" | "incomplete" | "weak"</span>

<span class="cm"># Cross-model alignment (FCAS) with null distribution</span>
gb2      = GlassboxV2(<span class="str">"gpt2-medium"</span>)
result2  = gb2.analyze(<span class="str">"When Mary and John went to the store, John gave a drink to"</span>,
                       <span class="str">"Mary"</span>, <span class="str">"John"</span>)
fcas_out = gb.functional_circuit_alignment(
    result[<span class="str">"circuit"</span>], result2[<span class="str">"circuit"</span>], k=<span class="num">10</span>
)
<span class="fn">print</span>(<span class="str">f"FCAS: {fcas_out['fcas']:.3f}  z={fcas_out['z_score']:.2f}"</span>)
</code></pre>

    <div class="disclosure" style="margin-top:20px;">
      <strong>ğŸ“– Full documentation</strong>
      See the
      <a href="https://github.com/designer-coderajay/Glassbox-AI-2.0-Mechanistic-Interpretability-tool"
         style="color:#60a5fa">GitHub README</a>
      for multi-prompt bootstrap CI examples, Logit Lens usage, and the Colab demo notebook.
    </div>
  </div>
</section>

<!-- â”€â”€ FEATURES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
<section>
  <div class="container">
    <h2>What Glassbox 2.0 Gives You</h2>
    <div class="feature-grid">
      <div class="card">
        <div class="card-icon">âš¡</div>
        <h3>3-Pass Attribution Scoring</h3>
        <p>Score every attention head in the model using a single clean forward pass,
           one corrupted forward pass, and one backward pass. Fast â€” no architectural
           changes needed.</p>
      </div>
      <div class="card">
        <div class="card-icon">ğŸ”</div>
        <h3>Minimum Faithful Circuit</h3>
        <p>Two-phase greedy search: forward selection by approximate sufficiency
           (no extra passes), backward pruning by exact comprehensiveness (2 passes
           per step). Returns the smallest circuit with F1 â‰¥ your threshold.</p>
      </div>
      <div class="card">
        <div class="card-icon">ğŸ“Š</div>
        <h3>Attention Heatmap</h3>
        <p>Per-layer, per-head attribution matrix visualised as an interactive
           heatmap. Spot dominant heads and suppression heads at a glance.</p>
      </div>
      <div class="card">
        <div class="card-icon">ğŸ”­</div>
        <h3>Logit Lens</h3>
        <p>Project the residual stream through the unembedding matrix at every
           layer to track how predictions crystallise from input to output.
           Method: nostalgebraist (2020).</p>
      </div>
      <div class="card">
        <div class="card-icon">ğŸ“</div>
        <h3>Bootstrap Confidence Intervals</h3>
        <p>Percentile bootstrap CIs over multiple prompts. Recommended n â‰¥ 20
           for reliable intervals. Seed-fixed for reproducibility.</p>
      </div>
      <div class="card">
        <div class="card-icon">ğŸ”—</div>
        <h3>Cross-Model FCAS</h3>
        <p>Functional Circuit Alignment Score with a null distribution from
           random circuit pairs â€” so you know if your alignment is actually
           meaningful or just noise.</p>
      </div>
    </div>
  </div>
</section>

<!-- â”€â”€ CITATIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
<section id="citations">
  <div class="container citations">
    <h2>References</h2>
    <ol>
      <li>
        <span>Wang et al. (2022)</span> â€” "Interpretability in the Wild: a Circuit
        for Indirect Object Identification in GPT-2 small."
        <em>arXiv:2211.00593</em>.
        Defines sufficiency, comprehensiveness, and the IOI task used throughout.
      </li>
      <li>
        <span>Nanda et al. (2023)</span> â€” "Attribution Patching: Activation
        Patching At Industrial Scale."
        <em>arXiv:2310.10348</em>.
        Source of the O(3) first-order Taylor approximation.
      </li>
      <li>
        <span>Conmy et al. (2023)</span> â€” "Towards Automated Circuit Discovery for
        Mechanistic Interpretability."
        <em>arXiv:2304.14997</em>.
        ACDC: the edge-level method Glassbox is compared against (37Ã— speedup).
      </li>
      <li>
        <span>Elhage et al. (2021)</span> â€” "A Mathematical Framework for
        Transformer Circuits."
        <em>Transformer Circuits Thread, Anthropic</em>.
        Theoretical foundation for circuit analysis and the residual stream view.
      </li>
      <li>
        <span>Geiger et al. (2021)</span> â€” "Causal Abstractions of Neural
        Networks."
        <em>NeurIPS 2021</em>.
        Causal basis for comprehensiveness via activation patching.
      </li>
      <li>
        <span>Goldowsky-Dill et al. (2023)</span> â€” "Localizing Model Behavior with
        Path Patching."
        <em>arXiv:2304.05969</em>.
        Extension of activation patching to path-level attribution.
      </li>
      <li>
        <span>nostalgebraist (2020)</span> â€” "Interpreting GPT: the Logit Lens."
        <em>LessWrong</em>.
        Method for projecting residual stream states into vocabulary space per layer.
      </li>
    </ol>
  </div>
</section>

<!-- â”€â”€ FOOTER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
<footer>
  <div class="container">
    <p style="margin-bottom:10px;">
      Glassbox 2.0 Â· Ajay Pravin Mahale Â· Hochschule Trier Â· MIT License Â· 2026
    </p>
    <p>
      <a href="https://github.com/designer-coderajay/Glassbox-AI-2.0-Mechanistic-Interpretability-tool">GitHub</a>
      &nbsp;Â·&nbsp;
      <a href="https://pypi.org/project/glassbox-mech-interp/">PyPI</a>
      &nbsp;Â·&nbsp;
      <a href="https://huggingface.co/spaces/designer-coderajay/Glassbox-ai">Live Demo</a>
    </p>
    <p style="margin-top:12px;font-size:0.78rem;color:#475569;">
      Sufficiency is a Taylor approximation. Comprehensiveness is exact.
      FCAS is a rank-aligned depth metric, not functional equivalence.
      Always interpret results alongside the null distribution baseline.
    </p>
  </div>
</footer>

</body>
</html>
